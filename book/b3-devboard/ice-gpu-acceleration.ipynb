{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmegankl/ece386-lab4/blob/main/book/b3-devboard/ice-gpu-acceleration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BpWg3hJENz0"
      },
      "source": [
        "# ICE 4: GPU Acceleration\n",
        "\n",
        "Let's check out some different PyTorch arithmetic on the CPU vs. GPU!\n",
        "\n",
        "We'll present the same code for both TensorFlow and Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kAYPwu1DhMNw",
        "outputId": "8da97899-2cad-4c6a-eaaf-32c5ae0a873d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ziTgDyKyhMN1"
      },
      "outputs": [],
      "source": [
        "%pip install -q tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUsKwMKeSSR5"
      },
      "source": [
        "First, import and check the version.\n",
        "\n",
        "We will also make sure we can access the CUDA GPU.\n",
        "This should always be your first step!\n",
        "\n",
        "We'll also set a manual_seed for the random operations. While this isn't strictly necessary for this experiment, it's good practice as it aids with reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ocKjJH0EYb9",
        "outputId": "fe1fab7f-6a2f-4544-d6c9-d6b24d2d28bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "\n",
        "# Help with reproducibility of test\n",
        "torch.manual_seed(2016)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise OSError(\"ERROR: No GPU found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q5OisWOehMN4",
        "outputId": "f91dba36-cc03-474b-a19a-353c89aad499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "Physical Devices Available:\n",
            " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# Help with reproducibility of test\n",
        "tf.random.set_seed(2016)\n",
        "\n",
        "# Make sure we can access the GPU\n",
        "print(\"Physical Devices Available:\\n\", tf.config.list_physical_devices())\n",
        "if not tf.config.list_physical_devices(\"GPU\"):\n",
        "    raise OSError(\"ERROR: No GPU found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JICBen1CSoPh"
      },
      "source": [
        "## Dot Product\n",
        "\n",
        "Dot products are **extremely** common tensor operations. They are used deep neural networks and linear algebra applications.\n",
        "\n",
        "A dot product is essentially just a bunch of multiplications and additions.\n",
        "\n",
        "- PyTorch provides the [`torch.tensordot()`](https://pytorch.org/docs/stable/generated/torch.tensordot.html) method.\n",
        "- TensorFlow provides the [`tf.tensordot()`](https://www.tensorflow.org/api_docs/python/tf/tensordot) method.\n",
        "\n",
        "First, let's define two methods to compute the dot product. One will take place on the CPU and the other on the GPU.\n",
        "\n",
        "### CPU Timing\n",
        "\n",
        "The CPU method is trivial!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C1eioHVdEJVg"
      },
      "outputs": [],
      "source": [
        "# Compute the tensor dot product on CPU\n",
        "def torch_cpu_dot_product(a, b):\n",
        "    return torch.tensordot(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GxWqqXRBhMN7"
      },
      "outputs": [],
      "source": [
        "# Compute the tensor dot product on CPU\n",
        "def tf_cpu_dot_product(a, b):\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        product = tf.tensordot(a, b, axes=2)\n",
        "    return product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8zwMrThMN7"
      },
      "source": [
        "### GPU Timing\n",
        "\n",
        "For **PyTorch** the GPU method has a bit more two it. We must:\n",
        "\n",
        "1. Send the tensors to the GPU for computation. We call torch.to() on the tensor to send it to a particular device\n",
        "2. Wait for the GPU to synchronize. According to the docs, GPU ops take place asynchronously so you need to use synchronize for precise timing.\n",
        "\n",
        "For **TensorFlow** the `tf.device` makes it a bit simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2ll905D2hMN8"
      },
      "outputs": [],
      "source": [
        "# Send the tensor to GPU then compute dot product\n",
        "# synchronize() required for timing accuracy, see:\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution\n",
        "def torch_gpu_dot_product(a, b):\n",
        "    a_gpu = a.to(\"cuda\")\n",
        "    b_gpu = b.to(\"cuda\")\n",
        "    product = torch.tensordot(a_gpu, b_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    return product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "trxhPH6VhMN8"
      },
      "outputs": [],
      "source": [
        "def tf_gpu_dot_product(a, b):\n",
        "    with tf.device(\"/GPU:0\"):\n",
        "        product = tf.tensordot(a, b, axes=2)\n",
        "    return product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyTqOmp9UBCO"
      },
      "source": [
        "### Running the benchmark\n",
        "\n",
        "This section declares the start and stop tensor sizes for our test.\n",
        "You can change `SIZE_LIMIT` and then run again; just know that at some point you will run out of memory!\n",
        "\n",
        "Next, it does tests at several sizes within this range, doubling each time.\n",
        "\n",
        "We use [`timeit.timeit()`](https://docs.python.org/3/library/timeit.html#timeit.timeit) for the tests. It will call the function multiple times and then average those times. Timeit is also more accurate than manually calling Python's time function and doing subtraction.\n",
        "\n",
        "Finally, results are saved into a list that's then exported to a pandas DataFrame for easy viewing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lrrGB5gshMN9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from timeit import timeit\n",
        "\n",
        "SIZE_LIMIT: int = 5000  # where to stop at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsN56pwmJDl3",
        "outputId": "aaf7042d-3701-4a9e-c814-235eb7ae368e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running PyTorch with 2D tensors from 10 to 5000 square\n",
            "PyTorch Results:\n",
            "    tensor_size  cpu_time  gpu_time  gpu_speedup\n",
            "0           100  0.001779  0.005502     0.323283\n",
            "1         12100  0.002094  0.007183     0.291463\n",
            "2         44100  0.005668  0.011524     0.491821\n",
            "3         96100  0.011940  0.018499     0.645414\n",
            "4        168100  0.020133  0.026561     0.757982\n",
            "5        260100  0.030350  0.043117     0.703912\n",
            "6        372100  0.045183  0.043348     1.042317\n",
            "7        504100  0.057984  0.057040     1.016549\n",
            "8        656100  0.079135  0.065701     1.204465\n",
            "9        828100  0.097341  0.085292     1.141266\n",
            "10      1020100  0.130673  0.097749     1.336820\n",
            "11      1232100  0.135504  0.116508     1.163041\n",
            "12      1464100  0.163814  0.129445     1.265516\n",
            "13      1716100  0.193714  0.151668     1.277226\n",
            "14      1988100  0.221213  0.175932     1.257379\n",
            "15      2280100  0.273726  0.200691     1.363917\n",
            "16      2592100  0.291131  0.228494     1.274129\n",
            "17      2924100  0.344517  0.252172     1.366203\n",
            "18      3276100  0.389864  0.284528     1.370215\n",
            "19      3648100  0.414630  0.345898     1.198709\n",
            "20      4040100  0.541971  0.358283     1.512690\n",
            "21      4452100  0.598638  0.419116     1.428337\n",
            "22      4884100  0.564612  0.420696     1.342092\n",
            "23      5336100  0.598771  0.460648     1.299845\n",
            "24      5808100  0.652541  0.495961     1.315711\n",
            "25      6300100  0.699788  0.546400     1.280724\n",
            "26      6812100  0.748314  0.592522     1.262931\n",
            "27      7344100  0.831774  0.634973     1.309935\n",
            "28      7896100  0.872087  0.677911     1.286432\n",
            "29      8468100  1.054342  0.807532     1.305635\n",
            "30      9060100  1.042201  0.784092     1.329182\n",
            "31      9672100  1.076495  0.842357     1.277955\n",
            "32     10304100  1.150472  0.899225     1.279405\n",
            "33     10956100  1.261818  0.968036     1.303483\n",
            "34     11628100  1.307601  1.092226     1.197189\n",
            "35     12320100  1.487636  1.081749     1.375214\n",
            "36     13032100  1.459567  1.134252     1.286811\n",
            "37     13764100  1.601122  1.197232     1.337353\n",
            "38     14516100  1.642119  1.337765     1.227510\n",
            "39     15288100  1.847128  1.328025     1.390883\n",
            "40     16080100  1.831587  1.405868     1.302815\n",
            "41     16892100  2.002796  1.458461     1.373226\n",
            "42     17724100  2.335003  1.551017     1.505465\n",
            "43     18576100  2.131363  1.619616     1.315968\n",
            "44     19448100  2.197059  1.714857     1.281190\n",
            "45     20340100  2.470709  1.768428     1.397121\n",
            "46     21252100  2.409079  1.844456     1.306119\n",
            "47     22184100  2.581465  2.072099     1.245822\n",
            "48     23136100  2.657810  2.006273     1.324750\n",
            "49     24108100  2.745708  2.179898     1.259558\n"
          ]
        }
      ],
      "source": [
        "# This cell is PyTorch\n",
        "tensor_size = 10  # start at size 10\n",
        "torch_results = []\n",
        "\n",
        "print(\"Running PyTorch with 2D tensors from\", tensor_size, \"to\", SIZE_LIMIT, \"square\")\n",
        "\n",
        "# Run the test\n",
        "while tensor_size < SIZE_LIMIT:\n",
        "    # Random array\n",
        "    a = torch.rand(tensor_size, tensor_size, device=\"cpu\")\n",
        "    b = torch.rand(tensor_size, tensor_size, device=\"cpu\")\n",
        "\n",
        "    # Time the CPU operation\n",
        "    cpu_time = timeit(\"torch_cpu_dot_product(a, b)\", globals=globals(), number=50)\n",
        "\n",
        "    # Time the GPU operation\n",
        "    # First, we send the data to the GPU, called the warm up\n",
        "    # It really depends on the application of this time is important or negligible\n",
        "    # We are doing it here becasue timeit() averages the results of multiple runs\n",
        "    torch_gpu_dot_product(a, b)\n",
        "    # Now we time the actual operation\n",
        "    gpu_time = timeit(\"torch_gpu_dot_product(a, b)\", globals=globals(), number=50)\n",
        "\n",
        "    # Record the results\n",
        "    torch_results.append(\n",
        "        {\n",
        "            \"tensor_size\": tensor_size * tensor_size,\n",
        "            \"cpu_time\": cpu_time,\n",
        "            \"gpu_time\": gpu_time,\n",
        "            \"gpu_speedup\": cpu_time / gpu_time,  # Greater than 1 means faster on GPU\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Increase tensor_size by 100. For larger SIZE_LIMITS, change to double tensor_size\n",
        "    # tensor_size = tensor_size * 2\n",
        "    tensor_size = tensor_size + 100\n",
        "\n",
        "# Done! Cast the results to a DataFrame and print\n",
        "torch_results_df = pd.DataFrame(torch_results)\n",
        "print(\"PyTorch Results:\")\n",
        "print(torch_results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "icfiKJL_hMN_",
        "outputId": "585e3d69-8f1b-44d4-b90f-5ee561feca7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running TensorFlow with 2D tensors from 10 to 5000 square\n",
            "TensorFlow Results:\n",
            "    tensor_size  cpu_time  gpu_time  gpu_speedup\n",
            "0           100  0.021958  0.010560     2.079401\n",
            "1         12100  0.010939  0.012063     0.906840\n",
            "2         44100  0.008504  0.009926     0.856779\n",
            "3         96100  0.009171  0.009697     0.945702\n",
            "4        168100  0.010602  0.009754     1.086999\n",
            "5        260100  0.012395  0.012611     0.982846\n",
            "6        372100  0.012976  0.009942     1.305107\n",
            "7        504100  0.014673  0.010148     1.445844\n",
            "8        656100  0.016733  0.009927     1.685570\n",
            "9        828100  0.020311  0.009953     2.040627\n",
            "10      1020100  0.022921  0.009927     2.308924\n",
            "11      1232100  0.030088  0.010424     2.886416\n",
            "12      1464100  0.027466  0.011424     2.404177\n",
            "13      1716100  0.030935  0.010406     2.972841\n",
            "14      1988100  0.034565  0.009819     3.520101\n",
            "15      2280100  0.038964  0.013049     2.986040\n",
            "16      2592100  0.042360  0.010540     4.019032\n",
            "17      2924100  0.049108  0.011033     4.450923\n",
            "18      3276100  0.052228  0.009948     5.249968\n",
            "19      3648100  0.057310  0.010430     5.494634\n",
            "20      4040100  0.061631  0.015614     3.947121\n",
            "21      4452100  0.067472  0.010131     6.660074\n",
            "22      4884100  0.073878  0.010489     7.043567\n",
            "23      5336100  0.079006  0.009760     8.094767\n",
            "24      5808100  0.088946  0.010058     8.842997\n",
            "25      6300100  0.093215  0.010809     8.623686\n",
            "26      6812100  0.100810  0.010910     9.240500\n",
            "27      7344100  0.118231  0.011454    10.322589\n",
            "28      7896100  0.113382  0.012617     8.986692\n",
            "29      8468100  0.121346  0.010679    11.362749\n",
            "30      9060100  0.130078  0.011261    11.550816\n",
            "31      9672100  0.143423  0.010162    14.113916\n",
            "32     10304100  0.151025  0.010397    14.526273\n",
            "33     10956100  0.163088  0.010289    15.851047\n",
            "34     11628100  0.163515  0.020056     8.152826\n",
            "35     12320100  0.174486  0.009852    17.711532\n",
            "36     13032100  0.182856  0.010028    18.234863\n",
            "37     13764100  0.204431  0.010324    19.801160\n",
            "38     14516100  0.203353  0.016240    12.521492\n",
            "39     15288100  0.249413  0.014039    17.766342\n",
            "40     16080100  0.253849  0.013420    18.915678\n",
            "41     16892100  0.278797  0.017477    15.952418\n",
            "42     17724100  0.245369  0.010682    22.970222\n",
            "43     18576100  0.273562  0.010679    25.617702\n",
            "44     19448100  0.278611  0.016757    16.626553\n",
            "45     20340100  0.338213  0.022844    14.805089\n",
            "46     21252100  0.541926  0.033957    15.958999\n",
            "47     22184100  0.594665  0.021118    28.158715\n",
            "48     23136100  0.682757  0.018375    37.156973\n",
            "49     24108100  0.545453  0.010894    50.071130\n"
          ]
        }
      ],
      "source": [
        "# This cell is TensorFlow\n",
        "tensor_size = 10  # start at size 10\n",
        "tf_results = []\n",
        "\n",
        "print(\n",
        "    \"Running TensorFlow with 2D tensors from\", tensor_size, \"to\", SIZE_LIMIT, \"square\"\n",
        ")\n",
        "\n",
        "# Run the test\n",
        "while tensor_size <= SIZE_LIMIT:\n",
        "    # Random tensor_size x tensor_size array\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        a = tf.random.uniform((tensor_size, tensor_size))\n",
        "        b = tf.random.uniform((tensor_size, tensor_size))\n",
        "\n",
        "    # Time the CPU operation\n",
        "    cpu_time = timeit(\"tf_cpu_dot_product(a, b)\", globals=globals(), number=10)\n",
        "\n",
        "    # Time the GPU operation\n",
        "    # First, we send the data to the GPU, called the warm up\n",
        "    # It really depends on the application of this time is important or negligible\n",
        "    # We are doing it here because timeit() runs the function multiple times anyway\n",
        "    tf_gpu_dot_product(a, b)\n",
        "    # Now we time the actual operation\n",
        "    gpu_time = timeit(\"tf_gpu_dot_product(a, b)\", globals=globals(), number=10)\n",
        "\n",
        "    # Record the results\n",
        "    tf_results.append(\n",
        "        {\n",
        "            \"tensor_size\": tensor_size * tensor_size,\n",
        "            \"cpu_time\": cpu_time,\n",
        "            \"gpu_time\": gpu_time,\n",
        "            \"gpu_speedup\": cpu_time / gpu_time,  # Greater than 1 means faster on GPU\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Increase tensor_size by 100. For larger SIZE_LIMITS, change to double tensor_size\n",
        "    # tensor_size = tensor_size * 2\n",
        "    tensor_size = tensor_size + 100\n",
        "\n",
        "# Done! Cast the results to a DataFrame and print\n",
        "tf_results_df = pd.DataFrame(tf_results)\n",
        "print(\"TensorFlow Results:\")\n",
        "print(tf_results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OXQbzUWUbqo"
      },
      "source": [
        "### Dot Product Results\n",
        "\n",
        "If you left the default sizes, you should see 10 rows of results.\n",
        "You'll notice that with small tensors the CPU is *faster* than the GPU!\n",
        "This is also indidcated by the **gpu_speedup** being less than 1.\n",
        "\n",
        "But as the tensor sizes grow, the GPU overtakes the CPU for speed! ğŸï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VBrJm0YQe7K"
      },
      "source": [
        "## Another Tensor Operation\n",
        "\n",
        "Your task is to repeat this benchmark below, but finding the minimum element in a **1D tensor**.\n",
        "You only need to do it with **one** framework.\n",
        "\n",
        "Use either\n",
        "\n",
        "- [`torch.min()`](https://pytorch.org/docs/stable/generated/torch.min.html) *or*\n",
        "- [`tf.math.reduce_min()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w0jcwTVFWhey"
      },
      "outputs": [],
      "source": [
        "# Define your methods here - using PyTorch\n",
        "def torch_cpu_min(a):\n",
        "    return torch.min(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qt8pxSE9WjdS",
        "outputId": "a3e17e1f-e4c8-414d-a571-05c0d3be1772",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running PyTorch with 1D tensors from 10 to 5000 square\n",
            "PyTorch Results:\n",
            "    tensor_size  cpu_time  gpu_time  gpu_speedup\n",
            "0           100  0.000670  0.000194     3.462049\n",
            "1         12100  0.000200  0.000186     1.072602\n",
            "2         44100  0.000188  0.000567     0.331661\n",
            "3         96100  0.000185  0.000175     1.058377\n",
            "4        168100  0.000115  0.000281     0.410569\n",
            "5        260100  0.000130  0.000108     1.198625\n",
            "6        372100  0.000120  0.000137     0.877037\n",
            "7        504100  0.000143  0.000109     1.313350\n",
            "8        656100  0.000108  0.000116     0.932640\n",
            "9        828100  0.000110  0.000136     0.810443\n",
            "10      1020100  0.000108  0.000109     0.994683\n",
            "11      1232100  0.000115  0.000108     1.058902\n",
            "12      1464100  0.000133  0.000110     1.210195\n",
            "13      1716100  0.000108  0.000107     1.006097\n",
            "14      1988100  0.000109  0.000108     1.009855\n",
            "15      2280100  0.000115  0.000108     1.063290\n",
            "16      2592100  0.000108  0.000114     0.943473\n",
            "17      2924100  0.000110  0.000120     0.920477\n",
            "18      3276100  0.000108  0.000107     1.003247\n",
            "19      3648100  0.000108  0.000109     0.986867\n",
            "20      4040100  0.000109  0.000119     0.915842\n",
            "21      4452100  0.000108  0.000107     1.005374\n",
            "22      4884100  0.000121  0.000107     1.123520\n",
            "23      5336100  0.000117  0.000108     1.088651\n",
            "24      5808100  0.000112  0.000108     1.031171\n",
            "25      6300100  0.000109  0.000108     1.012461\n",
            "26      6812100  0.000120  0.000108     1.113319\n",
            "27      7344100  0.000108  0.000112     0.962796\n",
            "28      7896100  0.000108  0.000108     0.995296\n",
            "29      8468100  0.000111  0.000107     1.038273\n",
            "30      9060100  0.000107  0.000117     0.910504\n",
            "31      9672100  0.000108  0.000121     0.892599\n",
            "32     10304100  0.000117  0.000107     1.089353\n",
            "33     10956100  0.000108  0.000109     0.986667\n",
            "34     11628100  0.000108  0.000123     0.876299\n",
            "35     12320100  0.000108  0.000108     1.003735\n",
            "36     13032100  0.000112  0.000108     1.041900\n",
            "37     13764100  0.000108  0.000117     0.920757\n",
            "38     14516100  0.000108  0.000108     1.003695\n",
            "39     15288100  0.000118  0.000107     1.106524\n",
            "40     16080100  0.000119  0.000108     1.100961\n",
            "41     16892100  0.000108  0.000106     1.017290\n",
            "42     17724100  0.000108  0.000108     1.004109\n",
            "43     18576100  0.000117  0.000108     1.082047\n",
            "44     19448100  0.000109  0.000114     0.953354\n",
            "45     20340100  0.000108  0.000107     1.006004\n",
            "46     21252100  0.000109  0.000108     1.004176\n",
            "47     22184100  0.000109  0.000114     0.958709\n",
            "48     23136100  0.000107  0.000117     0.914022\n",
            "49     24108100  0.000108  0.000108     0.999407\n"
          ]
        }
      ],
      "source": [
        "# Conduct your benchmark here\n",
        "\n",
        "tensor_size = 10  # start at size 10\n",
        "torch_results = []\n",
        "a = torch.randn(tensor_size, tensor_size, device=\"cpu\")\n",
        "\n",
        "print(\"Running PyTorch with 1D tensors from\", tensor_size, \"to\", SIZE_LIMIT, \"square\")\n",
        "\n",
        "# Run the test\n",
        "while tensor_size < SIZE_LIMIT:\n",
        "    # # Random array\n",
        "    # a = torch.rand(tensor_size, tensor_size, device=\"cpu\")\n",
        "    # b = torch.rand(tensor_size, tensor_size, device=\"cpu\")\n",
        "\n",
        "    # Time the CPU operation\n",
        "    cpu_time = timeit(\"torch_cpu_min(a)\", globals=globals(), number=50)\n",
        "\n",
        "    # Time the GPU operation\n",
        "    # First, we send the data to the GPU, called the warm up\n",
        "    # It really depends on the application of this time is important or negligible\n",
        "    # We are doing it here becasue timeit() averages the results of multiple runs\n",
        "    result = torch_cpu_min(a)\n",
        "    # Now we time the actual operation\n",
        "    gpu_time = timeit(\"torch_cpu_min(a)\", globals=globals(), number=50)\n",
        "\n",
        "    # Record the results\n",
        "    torch_results.append(\n",
        "        {\n",
        "            \"tensor_size\": tensor_size * tensor_size,\n",
        "            \"cpu_time\": cpu_time,\n",
        "            \"gpu_time\": gpu_time,\n",
        "            \"gpu_speedup\": cpu_time / gpu_time,  # Greater than 1 means faster on GPU\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Increase tensor_size by 100. For larger SIZE_LIMITS, change to double tensor_size\n",
        "    # tensor_size = tensor_size * 2\n",
        "    tensor_size = tensor_size + 100\n",
        "\n",
        "# Done! Cast the results to a DataFrame and print\n",
        "torch_results_df = pd.DataFrame(torch_results)\n",
        "print(\"PyTorch Results:\")\n",
        "print(torch_results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b0rM3aeWmcD"
      },
      "source": [
        "## Deliverable\n",
        "\n",
        "> **After** answering these questions, download this completed notebook and **upload to Gradescope.**\n",
        "\n",
        "### Reflection ğŸ“ˆ\n",
        "\n",
        "### *Why* does the CPU outperform the GPU dot product with smaller vectors?\n",
        "CPUs utilize the cache quickly and are made to do quick operations.\n",
        "\n",
        "### *How* did the CPU vs. GPU perform for `min()`?\n",
        "Initially, with smaller tensor sizes, the GPU outperformed the CPU by executing quicker. Hoever, with larger tensor sizes, the CPU was starting to compare in performance (and in some cases performed better).\n",
        "\n",
        "#### *Why* did it perform that way?\n",
        "I think this is because, as was mentioned earlier, the CPU utilizes the cache and, so it can pull up recent memory quickly. As such, the GPU was probably executing each computation individually, whereas the CPU had means for more efficient computations."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}